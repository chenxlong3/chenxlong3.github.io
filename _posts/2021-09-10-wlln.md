---
layout: post
title: 弱大数定律的推导及意义
date: 2021-09-10 12:47:46
tags: "Techniques"
mathjax: true
related_posts: false
---
样本数目足够多的情况下，样本均值会趋于期望。

样本数目足够多的情况下，事件发生的频率趋于概率。

为何两种说法都称为大数定律，样本均值与频率又到底是什么关系呢？

# 马尔可夫不等式 (The Markov Inequality)

Markov Inequality描述了一个非常简单的想法：如果随机变量恒大于零，且该随机变量的期望较小，那么我们随机做一次实验，所得到的$X$也不太可能会很大。举个例子，如果全国十四亿人口的平均身高为1.67米，那么我们随机抽一个人，他/她的身高不会偏离这个数字太远，他/她的身高为2米的概率是比较低的。

$P(X\geq a)$: 随机变量$X$不小于$a$ 的概率

$E[X]$: 随机变量$X$的期望

$a$: 任一正数

$$
P(X\geq a) \leq \frac{E[X]}{a}
$$

$Proof\ 1$:

$$
\begin{align*}
E[X] &= \int^{\infty}_{-\infty} xf(x)dx\\
&\geq \int^{\infty}_{a} xf(x)dx \\
&\geq \int^{\infty}_{a} af(x)dx \\
&= aP(X\geq a)
\\
\\
P(X\geq a) &\leq \frac{E[x]}{a}
\end{align*}
$$

除此之外，还有一种证明方法，但两者的内在思想是一样的，也在此简述一下。

$Proof\ 2$:

假设我们有另外一个随机变量$Y$，如下：

$$
\begin{equation*}
Y=\left\{
\begin{aligned}
a, &\quad X\geq a\\
0, &\quad X<a
\end{aligned}
\right.
\end{equation*}
$$

显然这是一个离散型随机变量，那么我们可以用以下公式计算出它的期望：

$$
\begin{align*}
E[Y] &= aP(X\geq a) + 0\cdot P(X<a)\\
&= aP(X\geq a)
\end{align*}
$$

因为当$X\geq a$时，$Y=a$，而当$X<a$时，$Y = 0$。所以很显然可以得到：$Y\leq X$。那么也就有$E[Y] \leq E[X]$。则：

$$
\begin{align*}
E[Y] = aP(&X\geq a) \leq E[X] \\
\\P(X\geq a) & \leq \frac{E[X]}{a}
\end{align*}
$$

可是稍微往深处想一想，就会觉得这个不等式非常鸡肋。从不等式上来看，随机变量大于或等于$k$倍期望的概率不超过$\frac{1}{k}$。这样说或许还不直观，我们再举回那个身高的例子，全国十四亿人口的平均身高为1.67米。这个不等式告诉我们，如果随机抽一个人，他/她的身高不小于3.34米的概率不超过$1/2$……

根据我们的常识，这个概率何止是不超过$1/2$，说是远小于1%也不为过。这其实就是这个Markov Inequality的一个缺点——它的上界定得太大了。


# 切比雪夫不等式 (The Chebyshev Inequality)

Chebyshev Inequality把方差也考虑了进来。这个不等式阐述的想法就是：如果一个随机变量的方差不大，那么这个随机变量不会离它的均值太远。不等式的形式如下：

$$
\begin{align*}
P(|X-\mu|\geq c)\leq \frac{\sigma^2}{c^2}
\end{align*}
$$

同样地，我们来证明一下这个不等式。

首先，绝对值这个东西有点碍眼，它的存在往往意味着分类讨论，我们可以先把它去掉，两边同时进行平方运算。

$$
\begin{align*}
P(|X-\mu|\geq c) = P((X-\mu)^2\geq c^2)
\end{align*}
$$

然后，把$(X-\mu)^2$看作一个新的随机变量，利用Markov Inequality得到，

$$
\begin{align*}
P(|X-\mu|\geq c) = P((X-\mu)^2 \geq c^2) &\leq \frac{E[(X-\mu)^2]}{c^2} = \frac{\sigma^2}{c^2}
\end{align*}
$$

证毕。

假设平均身高为167$cm$，方差为100$cm^2$，那么抽到一个人身高大于或等于3.34米的概率会小于或等于0.0035。这样看倒也科学不少。



# 弱大数定律 (Weak Law of Large Numbers, WLLN)

## 证明

接着我们来看一下这篇文章真正的主角——大数定律。更准确地说，是弱大数定律，也称为辛钦大数定律。至于强大数定律，会在这篇blog的结尾提一嘴，它的证明有点超出我的知识范畴了，我也就偷个懒吧。

先给出来大数定律的形式：

对于任意$\epsilon > 0$:

$$
\begin{align*}
P(|M_n-\mu|\geq \epsilon) \rightarrow 0, \ as\ n \rightarrow \infty
\end{align*}
$$

其中：

$M_n$: 样本均值，计算公式为

$$
M_n = \frac{X_1 + X_2+\cdots + X_n}{n}
$$

$\mu$: 随机变量的期望

$n$: 样本数量

$X_1\cdots X_n$: 独立同分布的随机变量



接下来我们给出弱大数定律的证明：

$Proof: $

根据均值及方差的线性性 (Linearity):


$$
\begin{align*}
E[M_n] &= E[\frac{X_1+\cdots+X_n}{n}]\\
\\
&= \frac{E[X_1]+\cdots+E[X_n]}{n}\\
\\
&=\frac{n\mu}{n}\\
\\
&= \mu\\
\\
\\
Var(M_n) &= Var(\frac{X_1+\cdots+X_n}{n})\\
\\
&= \frac{Var(X_1) + \cdots + Var(X_n)}{n^2}\\
\\
&= \frac{n\sigma^2}{n^2}\\
\\
&= \frac{\sigma^2}{n}
\end{align*}
$$


根据前面提到的Chebyshev Inequality，


$$
\begin{align*}
P(|M_n - \mu| \geq \epsilon) &\leq \frac{Var(M_n)}{\epsilon^2}
= \frac{\sigma^2}{n\epsilon^2}\\
\\
0\leq \lim_{n \to \infty} P(|M_n - \mu| &\geq \epsilon)\leq \lim_{n \to \infty} \frac{\sigma^2}{n\epsilon^2} = 0
\end{align*}
$$

根据夹逼定理，

$$
\begin{align*}
\lim_{n \to \infty} P(|M_n - \mu| &\geq \epsilon) = 0
\end{align*}
$$

证毕。

WLLN描述的实际上就是我们在开头讲的，样本数目足够多的情况下，样本均值会趋于期望。那这个描述样本均值的定律，是怎么和频率扯上关系的呢？


## 大数定律与事件概率估计

我们在讨论$X_1\cdots X_n$时，实际上讨论的是某一试验的多次重复。不妨用$X_i$来表示某事件$A$是否发生，即：

$$
\begin{equation}
X_i = \left\{
\begin{aligned}
1, \quad &\text{if event A happens}\\
0, \quad &\text{if event A doesn't happen}
\end{aligned}
\right.
\end{equation}
$$

那么，经过多次重复实验，得到的样本均值实际上就是事情发生的频率。

$$
\begin{align*}
M_n &= \frac{\sum^{n}_{i=1} X_i}{n} \\
&= \frac{\text{the number of times that event A happens}}{n}\\
&= frequency\\
\\
\mu &= 1\cdot P(A)+0\cdot (1-P(A))\\
\\
&=P(A)
\end{align*}
$$

所以，根据大数定律：

$$
\begin{align*}
\lim_{n \to \infty} P(|M_n - \mu| &\geq \epsilon) = \lim_{n \to \infty}P(frequency-P(A)) = 0
\end{align*}
$$
也就是说，频率趋于概率。

# 强大数定律

强大数定律的数学形式如下，

$$
\begin{align*}
P(\lim_{n \to \infty} |M_n - \mu| \geq \epsilon) &= 0
\end{align*}
$$

实际上就是把极限符号给放进去了，虽然只有一点变化，但是表示的意思就更强烈了一点。

> 弱大数定律指的是样本均值依概率收敛于期望，而强大数定律则指出样本均值几乎处处收敛于期望。

至于再深一点的理解，笔者也并不太了解，还需深入学习。