<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> 线性回归：机器学习的“Hello, World“ | Xiaolong Chen </title> <meta name="author" content="Xiaolong Chen"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.svg?169cce5d54035e520d22a1c86af66613"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://chenxlong3.github.io/blog/2021/linear-regression/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Xiaolong Chen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">Teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link"> <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">线性回归：机器学习的“Hello, World“</h1> <p class="post-meta"> Created on June 16, 2021 </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a>   ·   <a href="/blog/tag/learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Learning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="想要解决的问题">想要解决的问题</h1> <p>假设我们知道若干房子的面积以及对应的房价，那么如果随便告诉我们一套房子的面积，怎么才能知道这套房子最可能买多少钱呢？再假设我们知道了一些零件的使用时间和对应的维修费用，那么如果告诉我们一个零件的使用时间，我们要怎样推测出来对应的维修费用呢？没错，线性回归就是用来解决这些问题的，也就是通过拟合曲线来进行预测。</p> <h1 id="线性回归linear-regression">线性回归（Linear Regression）</h1> <p>线性回归大体分为三个部分，选择合适的模型（model），确定损失函数，梯度下降求解最优参数。</p> <h2 id="选择模型model">选择模型（Model）</h2> <p>首先我们要知道模型（model）是什么，在这个问题中，我们所说的model是指一个函数族。我们最终想要的结果，就是输入一个$x$，得到一个十分可信的输出$\hat{y}$，其实也就是找一个比较好的<strong>函数</strong>而已。问题是，这种函数可能有非常多，以单变量线性回归为例子，我们可以提出以下函数来进行拟合：</p> \[\begin{aligned} \hat{y} &amp;= \theta_0 + \theta_1 x \\ \hat{y} &amp;= \theta_0 + \theta_1 x + \theta_2 x^2\\ \hat{y} &amp;= \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3\\ \vdots \end{aligned}\] <p>同样地，我们挑一个最简单的，也就是第一个函数。</p> \[\hat{y} = \theta_0 + \theta_1 x\] <p>很显然，我们的最终目标就是找出$\theta_0$和$\theta_1$使得这个函数足够准确。那么问题又来了，我们要怎么样判断这个函数是否<strong>足够准确</strong>呢？</p> <h2 id="损失函数loss-function">损失函数（Loss function）</h2> <p>我们在说<strong>准确</strong>这个词的时候，实际上是在讨论什么？是<strong>误差</strong>。足够准确，也就是说误差足够小。损失函数（Loss function）就是用来描述这个误差的。如果要衡量误差，直接用预测值减去实际值就可以得出来了，如下：</p> \[\delta = \hat{y} - y\] <p>对于单个数据点我们当然可以这样做，那对于一堆数据点呢？一个很简单的想法就是直接将多个点的误差相加，问题是如果正的误差和负的误差相加起来抵消掉了怎么办。比如我们现在测量一个5cm的木块，第一次测出来是4.9cm，此时的误差就是-0.1cm，第二次测出来是5.1cm，此时的误差就是0.1cm。两次误差加起来就是0了，那岂不是相当于没有误差？那么现在聪明的你应该会想到，可以用绝对值。没错，这确实是一个可以选择的方式，然而，在实际应用中，带有绝对值的变量往往在求导等操作上会有一点麻烦。所以我们选用另一种方法，就是把这个数平方，即：</p> \[\delta = (\hat{y} - y)^2\] <p>同样，上式也是对一个数据点的计算，如果有多个数据点，假如一共有$m$个，直接相加的话，得到的就是所有数据点的误差之和了，但是当数据点非常多时，这个数字就变得很大了，为了方便计算，我们在前面再除以一个数据点的数量，也就得到了我们的损失函数（Loss function）：</p> \[L(\theta_0, \theta_1) = \frac{1}{m}\sum_{i=1}^m(\hat{y}^{(i)} - y)^2\] <p>（$\hat{y}^{(i)}$里的上标$(i)$表示第$i$个数据点） 好的，那么现在我们会发现数学符号逐渐多了起来，但是没有关系，我们把整个过程捋一捋。</p> <p>还记得我们的model吗？就是下面这个：</p> \[\hat{y} = \theta_0 + \theta_1 x\] <p>我们的任务是什么，就是找到最合适的$\theta_0$和$\theta_1$，使得上式可以取得较好的预测结果。怎么评估这种<strong>合适</strong>程度呢，我们将会用Loss函数来描述。不断改变$\theta_0$和$\theta_1$的值，利用手上已有的数据 ${ (x^{(1),} y^{(1)}), (x^{(2),} y^{(2)}), (x^{(3),} y^{(3)}), \dots, (x^{(m),} y^{(m)}) }$计算出Loss函数的值即可。那么什么时候才算是找到了最合适的$\theta_0$和$\theta_1$呢，显然，就是Loss函数最小的时候。误差最小就代表最准确，这是非常明显的。</p> <p>那么接下来，我们会介绍一种求解$\theta_0$和$\theta_1$的最常用的方法，梯度下降法（Gradient Descent）。</p> <h2 id="梯度下降法gradient-descent">梯度下降法（Gradient Descent）</h2> <p>经过高中数学的学习，想必大家都知道了导数的几何意义，在某一点的导数就等于过这一点切线的斜率，如下图所示：</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/linear-regression/LR1-480.webp 480w,/assets/img/posts/linear-regression/LR1-800.webp 800w,/assets/img/posts/linear-regression/LR1-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/posts/linear-regression/LR1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>显然，在$x_1$处的导数小于0，意味着在这个点的右边还有会使$y$更小的$x$值，暗示我们向右取值；相反地，在$x_2$处的导数大于0，意味着在这个点的左边有能使$y$更小的$x$值，暗示我们向左取值。所以就有如下的迭代策略：</p> \[\theta_j := \theta_j - \eta \frac{\partial}{\partial \theta_j}L(\theta_0, \theta_1) \ \ \ \ \ \ \ \ (j = 0, 1)\] <p>公式逐渐复杂了起来，不过问题不大，我们来分析一下各个参数的意义。</p> <p>$\theta_j$：就是代指$\theta_0$和$\theta_1$</p> <p>$:=$是<strong>赋值</strong>的意思，让左边的数等于右边的数</p> <p>$\eta$：学习率（Learning Rate），把降低Loss函数的过程比喻成下山，那么学习率$\eta$就是下山的步长，<del>步子跨大了容易扯着蛋</del>，步子跨大了就可能一步走到另外一边的上坡路去了，但是步子小了就会走得很慢。</p> <p>$\frac{\partial}{\partial \theta_j}L(\theta_0, \theta_1)$：Loss函数对$\theta_j$的偏导，可以大概理解为其余变量为常数时，对单变量的导数。</p> <p>总的来说，整个过程我们就是在变化$\theta_0, \theta_1$，利用训练集（Training Data）来让我们的“假想函数”与实际的差距越来越小。</p> <h2 id="求导公式">求导公式</h2> <p>我们这里把Loss函数的形式写完整：</p> \[L(\theta_0, \theta_1) = \sum_{i=1}^m(\hat{y}^{(i)} - y)^2 = \sum_{i=1}^m(\theta_0+\theta_1x^{(i)}-y^{(i)})^2\] <p>那么两个参数的偏导数就如下所示：</p> \[\frac{\partial}{\partial \theta_0}L(\theta_0, \theta_1)=\frac{2}{m} \sum_{i=1}^m(\theta_0+\theta_1x^{(i)}-y^{(i)})\] \[\frac{\partial}{\partial \theta_1}L(\theta_0, \theta_1)=\frac{2}{m} \sum_{i=1}^m(\theta_0+\theta_1x^{(i)}-y^{(i)}) \cdot x^{(i)}\] <h3 id="拓展幂次">拓展（幂次）</h3> <p>Loss函数构造的思想依然是一样的，只是我们选取的model发生变化了，当然，也就意味着偏导数的形式也发生变化了。假设我们一开始选取的model如下：</p> \[\hat{y} = \theta_0 + \theta_1 x + \theta_2 x^2 + \dots + \theta_n x^n\] <p>那么偏导数的形式变为：</p> \[\frac{\partial}{\partial \theta_0}L(\theta_0, \theta_1, \dots, \theta_n)=\frac{2}{m} \sum_{i=1}^m(\theta_0+\theta_1x^{(i)}+\dots + \theta_n x^{(i)n}-y^{(i)})\] \[\frac{\partial}{\partial \theta_j}L(\theta_0, \theta_1, \dots, \theta_n)=\frac{2}{m} \sum_{i=1}^m(\theta_0+\theta_1x^{(i)}+\dots + \theta_n x^{(i)n}-y^{(i)})\cdot x^{(i)j}\] <p>其中$j$取$1, 2, \dots, n$.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Xiaolong Chen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>